{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c07f9a",
   "metadata": {},
   "source": [
    "# SynCABEL: Results Analysis\n",
    "\n",
    "This notebook presents the comprehensive analysis of results for the **SynCABEL** (Synthetic Contextualized Augmentation for Biomedical Entity Linking) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b08f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Configure plotting for publication-ready figures\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ed0f2",
   "metadata": {},
   "source": [
    "## 1. Load Experimental Results\n",
    "\n",
    "Load the results from different experimental configurations and baseline comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core metric computation\n",
    "def _compute_metrics(df):\n",
    "    tp = df[\"success\"].sum()\n",
    "    fn = df[\"fail\"].sum()\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": recall,\n",
    "        \"Success\": int(tp),\n",
    "        \"Fail\": int(fn),\n",
    "        \"Support\": int(tp + fn),\n",
    "    }\n",
    "\n",
    "# Grouped metric computation (e.g. by category)\n",
    "def _compute_grouped_metrics(df, group_col):\n",
    "    grouped = df.group_by(group_col).agg([\n",
    "        pl.sum(\"success\").alias(\"Success\"),\n",
    "        pl.sum(\"fail\").alias(\"Fail\"),\n",
    "    ])\n",
    "    grouped = grouped.with_columns([\n",
    "        (pl.col(\"Success\") + pl.col(\"Fail\")).alias(\"Support\"),\n",
    "        (pl.col(\"Success\") / (pl.col(\"Success\") + pl.col(\"Fail\"))).fill_null(0.0).alias(\"Accuracy\"),\n",
    "    ])\n",
    "    return grouped.with_columns(Type=pl.col(group_col)).select([\"Type\", \"Accuracy\", \"Success\", \"Fail\", \"Support\"])\n",
    "\n",
    "# Main function\n",
    "def compute_metrics(result: pl.DataFrame, model, dataset):\n",
    "    # Global and conditional metrics\n",
    "    metrics_overall = _compute_metrics(result)\n",
    "    metrics_single_word = _compute_metrics(result.filter(~result[\"multi_word_mention\"]))\n",
    "    metrics_multi_word = _compute_metrics(result.filter(result[\"multi_word_mention\"]))\n",
    "    metrics_direct_match = _compute_metrics(result.filter(result[\"direct_match\"]))\n",
    "    metrics_undirect_match = _compute_metrics(result.filter(~result[\"direct_match\"]))\n",
    "\n",
    "    # Compile into a summary table\n",
    "    metrics_results = {\n",
    "        \"Single Word\": metrics_single_word,\n",
    "        \"Multiple Word\": metrics_multi_word,\n",
    "        \"Direct Match\": metrics_direct_match,\n",
    "        \"Undirect Match\": metrics_undirect_match,\n",
    "        \"Overall\": metrics_overall,\n",
    "    }\n",
    "\n",
    "    summary_df = pl.DataFrame(pd.DataFrame.from_dict(metrics_results, orient=\"index\").reset_index(names=\"Type\")).to_pandas()\n",
    "\n",
    "    # Compute metrics per category\n",
    "    per_category_df = _compute_grouped_metrics(result, \"category\").to_pandas()\n",
    "    result_df = pd.concat([summary_df, per_category_df])\n",
    "    result_df[\"model\"] = model\n",
    "    result_df[\"dataset\"] = dataset\n",
    "    result_df[\"max_length\"] = None\n",
    "    result_df[\"n_beams\"] = None\n",
    "    result_df[\"constrained_inference\"] = None\n",
    "    result_df[\"training_stop\"] = None\n",
    "    result_df[\"data_augmentation\"] = \"No augmentation\"\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental results\n",
    "def load_results(results_dir: str = \"data/results\"):\n",
    "    \"\"\"\n",
    "    Load experimental results from JSON files\n",
    "    \"\"\"\n",
    "    all_results_df = pd.read_pickle(f\"{results_dir}/gen_results.pkl\")\n",
    "    all_dfs = []\n",
    "    models = [\"coder-all\", \"SapBERT\", \"scispacy\"]\n",
    "    datasets = [\"emea\", \"medline\", \"MM\"]\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            raw_results = pl.read_parquet(f\"{results_dir}/{model}/{dataset}_all_results.parquet\")\n",
    "            all_dfs.append(compute_metrics(raw_results, model, dataset))\n",
    "    all_results_df = pd.concat([all_results_df] + all_dfs)\n",
    "    return all_results_df\n",
    "\n",
    "# Load all results\n",
    "results = load_results()\n",
    "print(f\"\\nðŸ“ˆ Loaded results for {len(results)} experimental configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b79866",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison\n",
    "\n",
    "Compare the performance of baseline methods against SynCABEL across both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89458e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = results[(results.Type == \"Overall\") & (results.data_augmentation.isin([\"No augmentation\", \"LLM augmented complete\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18372536",
   "metadata": {},
   "source": [
    "## 3. Visualization of Results\n",
    "\n",
    "Create visualizations comparing baseline and SynCABEL performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Model\"] = results[\"model\"] + \"-\" + results[\"data_augmentation\"]\n",
    "results[\"Model\"] = results[\"Model\"].replace({\n",
    "    \"coder-all-No augmentation\": \"CODER\",\n",
    "    \"SapBERT-No augmentation\": \"SapBERT\",\n",
    "    \"mt5-large-No augmentation\": \"SapBERT\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable correct data types\n",
    "all_results_df = results[results.data_augmentation.isin([\"LLM augmented complete\", \"No augmentation\"])]\n",
    "all_results_df[\"max_length\"] = all_results_df[\"max_length\"].astype(str)\n",
    "all_results_df[\"n_beams\"] = all_results_df[\"n_beams\"].astype(str)\n",
    "\n",
    "# Create selection widgets for fixed filters\n",
    "type_options = sorted(all_results_df[\"Type\"].unique())\n",
    "dataset_options = sorted(all_results_df[\"dataset\"].unique())\n",
    "\n",
    "dropdown_type = alt.binding_select(options=type_options, name=\"Semantic Type: \")\n",
    "dropdown_dataset = alt.binding_select(options=dataset_options, name=\"Dataset: \")\n",
    "\n",
    "\n",
    "type_selector = alt.selection_point(\n",
    "    fields=[\"Type\"],\n",
    "    bind=dropdown_type,\n",
    "    value=str(type_options[0])  # Set to first option by default\n",
    ")\n",
    "dataset_selector = alt.selection_point(\n",
    "    fields=[\"dataset\"],\n",
    "    bind=dropdown_dataset,\n",
    "    value=dataset_options[1]\n",
    ")\n",
    "\n",
    "\n",
    "# Base chart with dynamic encoding\n",
    "viz = alt.Chart(results).add_selection(\n",
    "    type_selector,\n",
    "    dataset_selector,\n",
    ").transform_filter(\n",
    "    type_selector & dataset_selector\n",
    ").transform_joinaggregate(\n",
    "    max_accuracy='max(Accuracy)',\n",
    "    groupby=['model', 'data_augmentation']\n",
    ").transform_filter(\n",
    "    'datum.Accuracy === datum.max_accuracy'\n",
    ").mark_bar().encode(\n",
    "    # Dynamic x-axis based on selection\n",
    "    x=alt.X(\"model:N\", title=\"Model\"),\n",
    "    y=alt.Y(\"Accuracy:Q\", title=\"Recall\", scale=alt.Scale(domain=[0, 1])).stack(None),\n",
    "    # Dynamic column grouping\n",
    "    column=alt.Column('data_augmentation:N', title=\"Data Augmentation\"),\n",
    "    color=alt.Color('data_augmentation:N', legend=None),\n",
    "    tooltip=[\"dataset\", \"Type\", \"model\", \"Accuracy\", \"Success\", \"Fail\", \"Support\",\n",
    "             \"max_length\", \"n_beams\", \"constrained_inference\", \"data_augmentation\"]\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ").interactive()\n",
    "\n",
    "viz"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
